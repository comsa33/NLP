{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0eaef8-eb92-4c41-a97c-14462262d0da",
   "metadata": {},
   "source": [
    "[출처]https://wikidocs.net/69141"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3016bc0-668a-49eb-a07a-371ab331da38",
   "metadata": {},
   "source": [
    "네거티브 샘플링(Negative Sampling)을 사용하는 Word2Vec을 직접 케라스(Keras)를 통해 구현해봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751e9d9-447f-40bc-a8ce-f5afc45e66ce",
   "metadata": {},
   "source": [
    "# 1. 네거티브 샘플링(Negative Sampling)\n",
    "----------\n",
    "Word2Vec의 출력층에서는 소프트맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트합니다. 만약 단어 집합의 크기가 수만 이상에 달한다면 이 작업은 굉장히 무거운 작업이므로, Word2Vec은 꽤나 학습하기에 무거운 모델이 됩니다.\n",
    "\n",
    "Word2Vec은 역전파 과정에서 모든 단어의 임베딩 벡터값의 업데이트를 수행하지만, 만약 현재 집중하고 있는 중심 단어와 주변 단어가 '강아지'와 '고양이', '귀여운'과 같은 단어라면, 사실 이 단어들과 별 연관 관계가 없는 '돈가스'나 '컴퓨터'와 같은 수많은 단어의 임베딩 벡터값까지 업데이트하는 것은 비효율적입니다.\n",
    "\n",
    "네거티브 샘플링은 Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법입니다. 가령, 현재 집중하고 있는 주변 단어가 '고양이', '귀여운'이라고 해봅시다. 여기에 '돈가스', '컴퓨터', '회의실'과 같은 단어 집합에서 무작위로 선택된 주변 단어가 아닌 단어들을 일부 가져옵니다. 이렇게 하나의 중심 단어에 대해서 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 변환합니다. 주변 단어들을 긍정(positive), 랜덤으로 샘플링 된 단어들을 부정(negative)으로 레이블링한다면 이진 분류 문제를 위한 데이터셋이 됩니다. 이는 기존의 단어 집합의 크기만큼의 선택지를 두고 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f6ef5-72a3-47a3-9c83-31ae6206e7dd",
   "metadata": {},
   "source": [
    "# 3. 20뉴스그룹 데이터 전처리하기\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c23d1e-ef34-4380-8ecd-8c447078d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bbad9-a1f4-476d-85c1-93a95a1562fd",
   "metadata": {},
   "source": [
    "20newsgroups 데이터를 사용합니다. \n",
    "이번 실습에서는 하나의 샘플에 최소 단어 2개는 있어야 합니다.\n",
    "그래야만 중심 단어, 주변 단어의 관계가 성립하며 그렇지 않으면 샘플을 구성할 수 없어 에러가 발생합니다.\n",
    "전처리 과정에서 지속적으로 이를 만족하지 않는 샘플들을 제거하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc825a66-0249-48ac-ad44-698303c0b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "                             random_state=1, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수 :', len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9fa61-c60b-4d37-9721-025dae47b165",
   "metadata": {},
   "source": [
    "총 샘플수는 11,314개 입니다.\n",
    "전처리를 진행해봅시다.\n",
    "불필요한 토큰을 제거하고, 소문자화를 통해 정규화를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3f8559-ad36-4d01-8895-3b00812fbdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/rnj5svk11p1157tkdv28yb6r0000gn/T/ipykernel_82368/3942184766.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        Well i m not sure about the story nad it did s...\n",
       "1               Yeah  do you expect people to read the ...\n",
       "2        Although I realize that principle is not one o...\n",
       "3        Notwithstanding all the legitimate fuss about ...\n",
       "4        Well  I will have to change the scoring on my ...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein  an Israeli journalist  will ...\n",
       "11310                                                     \n",
       "11311     I agree   Home runs off Clemens are always me...\n",
       "11312    I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                                 N...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df['clean_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f559ecc-b5cf-4636-a367-958def2b304b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Well sure about story seem biased What disagre...\n",
       "1        Yeah expect people read actually accept hard a...\n",
       "2        Although realize that principle your strongest...\n",
       "3        Notwithstanding legitimate fuss about this pro...\n",
       "4        Well will have change scoring playoff pool Unf...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein Israeli journalist will speak...\n",
       "11310                                                     \n",
       "11311    agree Home runs Clemens always memorable Kinda...\n",
       "11312    used DeskJet with Orange Micros Grappler Syste...\n",
       "11313    argument with Murphy scared hell when came las...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "news_df['clean_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df6ad8f-e73c-43b3-9d74-dd90473e002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well sure about story seem biased what disagre...\n",
       "1        yeah expect people read actually accept hard a...\n",
       "2        although realize that principle your strongest...\n",
       "3        notwithstanding legitimate fuss about this pro...\n",
       "4        well will have change scoring playoff pool unf...\n",
       "                               ...                        \n",
       "11309    danny rubenstein israeli journalist will speak...\n",
       "11310                                                     \n",
       "11311    agree home runs clemens always memorable kinda...\n",
       "11312    used deskjet with orange micros grappler syste...\n",
       "11313    argument with murphy scared hell when came las...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "news_df['clean_doc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc4767-9a31-47af-9f24-223c6c5fe06f",
   "metadata": {},
   "source": [
    "현재 데이터프레임에 Null값이 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14472097-526e-4d6e-8971-6dd47285d911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c79d0-1a88-4db0-8290-d8125fe865b0",
   "metadata": {},
   "source": [
    "Null 값이 없지만, 빈값(empty) 유무도 확인해야 합니다.\n",
    "모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3222536-8586-4866-a329-83bb077dc6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "539a8a6e-aec8-46b8-bda0-3e4cfe1d0ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document     218\n",
       "clean_doc    319\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e26b5e-fa7c-4825-97df-7222ca2e1a7e",
   "metadata": {},
   "source": [
    "Null 값이 있는 것을 확인했습니다. Null 값을 제거합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1632894-c6d1-4940-be89-bfc4a8c0cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10995\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :', len(news_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6e731-5d9e-4ee2-9be2-348a7628ffd3",
   "metadata": {},
   "source": [
    "샘플 수가 일부 줄어든 것을 확인할 수 있습니다. NLTK 에서 정의한 불용어 리스트를 사용하여 불용어를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45c1d443-c028-4be4-8d6d-6d807a6b21ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ruo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [well, sure, about, story, seem, biased, what,...\n",
       "1        [yeah, expect, people, read, actually, accept,...\n",
       "2        [although, realize, that, principle, your, str...\n",
       "3        [notwithstanding, legitimate, fuss, about, thi...\n",
       "4        [well, will, have, change, scoring, playoff, p...\n",
       "                               ...                        \n",
       "11308    [sunroof, leaks, always, thought, those, thing...\n",
       "11309    [danny, rubenstein, israeli, journalist, will,...\n",
       "11311    [agree, home, runs, clemens, always, memorable...\n",
       "11312    [used, deskjet, with, orange, micros, grappler...\n",
       "11313    [argument, with, murphy, scared, hell, when, c...\n",
       "Name: clean_doc, Length: 10995, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어를 제거\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')   # from nltk.corpus import stopwords\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77378d6d-a36a-4ed1-97a4-0c2e336babb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased, disagree, st...\n",
       "1        [yeah, expect, people, read, actually, accept,...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4        [well, change, scoring, playoff, pool, unfortu...\n",
       "                               ...                        \n",
       "11308    [sunroof, leaks, always, thought, things, roya...\n",
       "11309    [danny, rubenstein, israeli, journalist, speak...\n",
       "11311    [agree, home, runs, clemens, always, memorable...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy, scared, hell, came, last, y...\n",
       "Name: clean_doc, Length: 10995, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "584c20d0-c712-4034-87d0-8c17e0bf452b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8478e7e-a097-4e84-a864-f5c83a5f872d",
   "metadata": {},
   "source": [
    "불용어를 제거하였으므로 단어의 수가 줄어들었습니다. 모든 샘플 중 단어가 1개 이하인 경우를 모두 찾아 제거하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0ba910b-8006-4920-87b0-3ddc347e5061",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44,\n",
       " 260,\n",
       " 353,\n",
       " 1651,\n",
       " 1839,\n",
       " 2321,\n",
       " 2336,\n",
       " 2371,\n",
       " 2862,\n",
       " 2963,\n",
       " 3290,\n",
       " 3387,\n",
       " 3395,\n",
       " 3396,\n",
       " 3421,\n",
       " 3563,\n",
       " 3591,\n",
       " 3713,\n",
       " 3874,\n",
       " 3897,\n",
       " 4180,\n",
       " 4524,\n",
       " 4587,\n",
       " 4617,\n",
       " 4947,\n",
       " 4970,\n",
       " 5129,\n",
       " 5525,\n",
       " 6015,\n",
       " 6227,\n",
       " 6652,\n",
       " 6723,\n",
       " 6883,\n",
       " 7080,\n",
       " 7956,\n",
       " 8000,\n",
       " 8156,\n",
       " 8212,\n",
       " 8283,\n",
       " 8588,\n",
       " 8867,\n",
       " 8903,\n",
       " 9045,\n",
       " 9555,\n",
       " 9696,\n",
       " 10439,\n",
       " 10447,\n",
       " 10564,\n",
       " 10707,\n",
       " 10730,\n",
       " 10750,\n",
       " 10838,\n",
       " 10896,\n",
       " 10908,\n",
       " 10967]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <=1]\n",
    "drop_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "491531f9-a5c3-45fd-84b3-5df87cc8f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf2/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
    "print('총 샘플 수 :', len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9ed37-2e55-41a2-ab53-4ea3137d630a",
   "metadata": {},
   "source": [
    "샘플 수가 다시 줄어들었습니다. 단어 집합을 생성하고, 정수 인코딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd67924-1185-4bd2-874e-b1cbe966ee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x2c3bb5310>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afb34a79-d5fa-42e0-b252-d2a67860732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "type(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4e49caf-845b-4b35-ba3b-adf638df61d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "type(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59e9ae3d-d29c-4afa-a1ad-cc6a1d00659f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87040b6-8f2c-43a6-84d4-8567924e1a8a",
   "metadata": {},
   "source": [
    "단어 집합의 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9ad7e4d-da0a-4f87-b574-8ca287f2fe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 64277\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "print('단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1bba7-c5b7-43c1-af8f-a7dba1a443e8",
   "metadata": {},
   "source": [
    "# 4. 네거티브 샘플링을 통한 데이터셋 구성하기\n",
    "---------\n",
    "토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정을 거쳤습니다. 네거티브 샘플링을 통한 데이터셋을 구성할 차례입니다. 이를 위해서는 네거티브 샘플링을 위해서 케라스에서 제공하는 전처리 도구인 skipgrams를 사용합니다. 어떤 전처리가 수행되는지 그 결과를 확인하기 위해서 (꽤 시간이 소요되는 작업이므로) 상위 10개의 뉴스그룹 샘플에 대해서만 수행해봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e61184c-fc67-4ee0-b1d7-c243c46a21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "# 네거티브 샘플링\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27edb0-ff28-4750-802f-2b60bcb49688",
   "metadata": {},
   "source": [
    "결과를 확인합니다. 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f01bc67-097e-425c-9d0d-cbee21ba4719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ignore (1979), subsidizing (15228)) -> 1\n",
      "(well (9), radius (2233)) -> 0\n",
      "(soldiers (957), away (178)) -> 1\n",
      "(unfortunate (4295), look (66)) -> 1\n",
      "(away (178), portuguese (19922)) -> 0\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플인 skip_grams[0] 내 skipgrams 로 형성된 데이터 셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff1dff-6b00-41d2-87e9-5383c0c84e9a",
   "metadata": {},
   "source": [
    "윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 그렇지 않은 경우는 0의 레이블을 가지도록 하여 데이터셋을 구성합니다. 이 과정은 각각의 뉴스그룹 샘플에 대해서 동일한 프로세스로 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc3c108f-80b8-4eee-be2d-af3010de114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :', len(skip_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50359f8-8aae-4ac9-a3be-63e946ec0a23",
   "metadata": {},
   "source": [
    "encoded 중 상위 10개의 뉴스그룹 샘플에 대해서만 수행하였으므로 10이 출력됩니다.\n",
    "그리고 10개의 뉴스그룹 샘플 각각은 수많은 중심 단어, 주변 단어의 쌍으로 된 샘플들을 갖고 있습니다. \n",
    "첫번째 뉴스그룹 샘플이 가지고 있는 pairs와 labels의 개수를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edc6bee7-09d8-499c-a49c-7ed6263e808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "# 첫번쨰 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4da23-663d-48d5-9f1a-9aa340cfcc8e",
   "metadata": {},
   "source": [
    "이 작업을 모든 뉴스 그룹 샘플에 대해서 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a580d22-bd80-4480-a099-dffc4e08d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb50c5-8d9b-4c16-9238-8f7c0e5eaf0d",
   "metadata": {},
   "source": [
    "# 5.Skip-Gram with Negative Sampling(SGNS) 구현하기\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52072f03-a669-4957-88a4-70ba3e68399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c849e9-db48-4a49-8f68-3b5b11fe154f",
   "metadata": {},
   "source": [
    "하이퍼파라미터인 임베딩 벡터의 차원은 100으로 정하고, 두 개의 임베딩 층을 추가합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26f7585c-3a3c-4e80-9053-45db0bdf600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6049709-f0a8-4c7c-9018-a613156a9994",
   "metadata": {},
   "source": [
    "각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블이며 각 단어는 임베딩 테이블을 거쳐서 내적을 수행하고, 내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26a967b2-821b-4362-827f-636aafa2fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1, 100)       6427700     ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1, 100)       6427700     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, 1, 1)         0           ['embedding_4[0][0]',            \n",
      "                                                                  'embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1)            0           ['dot_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 1)            0           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17924ba9-51b0-4572-a41d-3d510d628157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 22:59:28.133217: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-27 22:59:28.384822: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-27 22:59:28.639861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 4620.764626778662\n",
      "Epoch : 2 Loss : 3664.4258460476995\n",
      "Epoch : 3 Loss : 3489.8834881819785\n",
      "Epoch : 4 Loss : 3279.878042751923\n",
      "Epoch : 5 Loss : 3052.221312460024\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b040d39-7398-4300-8949-53d05af084bc",
   "metadata": {},
   "source": [
    "# 6. 결과 확인하기\n",
    "학습된 모델의 결과를 확인해보겠습니다. 학습된 임베딩 벡터들을 vector.txt에 저장합니다. 그 후 이를 gensim의 models.KeyedVectors.load_word2vec_format()으로 로드하면 쉽게 단어 벡터 간 유사도를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "083bf638-6f6f-4451-9245-70f5835a3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embed_size = 100\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "# 모델 로드\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7172dde-f364-4aa7-9044-a4daa5d36441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wounded', 0.8767865896224976),\n",
       " ('occupying', 0.819762110710144),\n",
       " ('moslems', 0.8183746933937073),\n",
       " ('murdered', 0.8068832159042358),\n",
       " ('irgun', 0.8020448088645935),\n",
       " ('massacred', 0.8015314936637878),\n",
       " ('slaughtered', 0.7968186140060425),\n",
       " ('flee', 0.7929799556732178),\n",
       " ('israelis', 0.7892656922340393),\n",
       " ('civilians', 0.788791835308075)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a137cfac-3aad-4fc1-b709-c4634a687cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hospital', 0.6016629934310913),\n",
       " ('pain', 0.5619372129440308),\n",
       " ('clinic', 0.5574035048484802),\n",
       " ('symptoms', 0.5224595069885254),\n",
       " ('infection', 0.5112951397895813),\n",
       " ('seizures', 0.5079503059387207),\n",
       " ('patients', 0.5064199566841125),\n",
       " ('orthopedic', 0.5045276880264282),\n",
       " ('patient', 0.500247597694397),\n",
       " ('chemotherapy', 0.49900224804878235)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d4e33e7-d57b-4451-b493-d074053f14f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ruled', 0.596937894821167),\n",
       " ('committee', 0.5883607268333435),\n",
       " ('represent', 0.5783965587615967),\n",
       " ('whites', 0.5744640231132507),\n",
       " ('lawyers', 0.5617749094963074),\n",
       " ('established', 0.5598101019859314),\n",
       " ('enforcement', 0.5584421157836914),\n",
       " ('filed', 0.5521964430809021),\n",
       " ('presidential', 0.5489435195922852),\n",
       " ('specter', 0.5455091595649719)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88281b60-8b46-4abb-9fef-9179eb324fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jury', 0.7068297266960144),\n",
       " ('surrounded', 0.6640816330909729),\n",
       " ('exodus', 0.6599657535552979),\n",
       " ('fanatical', 0.6575828790664673),\n",
       " ('stalin', 0.6563337445259094),\n",
       " ('saudi', 0.6527383923530579),\n",
       " ('homosexuals', 0.6504783034324646),\n",
       " ('knives', 0.6490634679794312),\n",
       " ('tactic', 0.6480515003204346),\n",
       " ('bombing', 0.6475279927253723)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['knife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50f06bab-b476-42bb-832d-b4448c736981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cylinder', 0.6270935535430908),\n",
       " ('rear', 0.6258969902992249),\n",
       " ('tires', 0.602568507194519),\n",
       " ('honda', 0.5937182903289795),\n",
       " ('aluminum', 0.5907703638076782),\n",
       " ('toyota', 0.5874179005622864),\n",
       " ('brake', 0.5856103301048279),\n",
       " ('torque', 0.5842881798744202),\n",
       " ('gasoline', 0.5831610560417175),\n",
       " ('valve', 0.5759072303771973)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['engine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce9f80-8f93-4da4-8876-3f8455455a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
